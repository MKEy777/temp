# 项目文档 

## 1. 概述与核心思想

本项目是一个基于 C++ 实现的高性能网络服务器框架。其核心是采用了 **Reactor 设计模式**，并结合**多线程**技术，构建了一个 **“主-从 Reactor”（Main-Sub Reactor）** 的并发模型，同时引入**线程池**来处理业务逻辑，从而实现了 I/O 处理与业务计算的分离。

这种设计的核心思想在于，彻底贯彻“事件驱动”的编程范式。主线程（Main Reactor）只负责一件最重要且最快速的事情：监听并接受新的客户端连接。一旦连接建立，后续所有的 I/O 操作（数据的读取和发送）全部交由子线程（Sub Reactor）处理。而真正耗时的业务逻辑计算，则被再次从 I/O 线程中剥离，扔进一个独立的线程池中执行。

通过这种层层分工，每个线程的职责都变得非常单一，避免了任何一个环节的阻塞影响到整个服务的响应能力，从而最大限度地提升了系统的吞吐量和并发处理能力。

## 2. 宏观架构：三大核心组件的协同

从整体上看，整个系统由三个关键部分协同工作：**Main Reactor**、**Sub Reactor** 和 **ThreadPool**。

一个典型的客户端请求处理流程如下：

1. **接受连接**：客户端发起连接请求。**Main Reactor** 在其独立的线程中运行，通过底层的 I/O 多路复用机制（如 Epoll）监听到这个连接事件。
2. **分发连接**：`ListenHandler`（运行在 Main Reactor 中）被唤醒，调用 `accept()` 函数接受新连接，并创建一个代表该连接的 `SockHandler`。紧接着，它会将这个新的 `SockHandler` **注册**到某个 **Sub Reactor** 上。
3. **处理 I/O**：这个连接后续的所有 I/O 事件（如客户端发送数据）都将在那个被选中的 **Sub Reactor** 线程中被监听和处理。当数据到达时，`SockHandler` 的 `handle_read()` 方法被调用。
4. **业务处理**：`SockHandler` 从 socket 中读取数据后，并**不**直接进行处理，而是将数据和处理逻辑打包成一个任务，提交给 **ThreadPool**。I/O 线程（Sub Reactor 线程）的使命到此暂时完成，它会立刻返回去处理其他连接上的 I/O 事件，不会被业务计算所阻塞。
5. **响应数据**：线程池中的某个工作线程获取到任务，执行具体的业务逻辑。处理完成后，工作线程会准备好待发送的响应数据，并通知对应的 Sub Reactor：“嘿，数据准备好了，可以发送了”。这个通知是通过调用 `reactor->modify()` 方法，将该连接的监听事件从“可读”修改为“可写”来完成的。
6. **发送响应**：Sub Reactor 监听到“可写”事件，调用 `SockHandler` 的 `handle_write()` 方法，将处理好的数据发送回客户端。发送完毕后，再次调用 `reactor->modify()` 将事件修改回“可读”，等待客户端的下一次请求。

## 3. 微观实现：代码世界的深度游览

接下来，我们将潜入代码的细节，看看上述的宏观架构是如何通过一行行代码构建起来的。

### 3.1. `main.cpp`：程序的起点

一切故事都从 `main` 函数开始。这个文件是整个程序的入口，它的职责非常简单：解析命令行参数，然后决定是启动服务端，还是启动客户端。

```
// 来自 main.cpp
#include <iostream>
#include <string.h>

int run_server();
int run_client();

int main(int argc, char* argv[]) {
    if (argc != 2) {
        std::cerr << "Usage: " << argv[0] << " [s | c]" << std::endl;
        return 1;
    }

    if (strcmp(argv[1], "s") == 0) {
        return run_server();
    }
    else if (strcmp(argv[1], "c") == 0) {
        return run_client();
    }
    else {
        std::cerr << "Invalid argument. Use 's' for server or 'c' for client." << std::endl;
        return 1;
    }
}
```

代码解读:

这段代码是一个典型的命令行程序入口。它检查传入的参数数量，如果不是两个（程序名 + 参数），就打印用法提示并退出。然后，它使用 strcmp 比较第二个参数是 "s"还是 "c"，并据此调用在其他文件中声明的 run_server() 或 run_client() 函数。它像一个交通警察，指挥着程序的走向。

### 3.2. `server.cpp`：宏伟蓝图的组装车间

如果说 `main.cpp` 是大门，那么 `server.cpp` 就是进入服务端世界后看到的总装车间。这里，我们亲手将之前讨论的各大核心组件（Main Reactor, Sub Reactor, ThreadPool）一个个实例化，并把它们正确地连接起来，构成我们设计的宏伟蓝图。

```
// 来自 server.cpp
int run_server() {
    // 1. 创建一个拥有4个工作线程的线程池
    thread_pool = new ThreadPool(4);

    // 2. 创建 Sub Reactor，并为它启动一个独立的子线程
    Reactor* sub_reactor = new Reactor();
    std::thread sub_thread([sub_reactor]() {
        sub_reactor->event_loop(100);
        });

    // 3. 创建 Main Reactor，它将运行在当前主线程
    Reactor* main_reactor = new Reactor();
    
    // 4. 创建监听处理器，并将 sub_reactor 的指针传给它
    ListenHandler* acceptor = new ListenHandler(9527, sub_reactor);
    
    // 5. 将监听处理器注册到 Main Reactor，监听新连接事件
    main_reactor->regist(acceptor, READ);
    
    // 6. Main Reactor 进入事件循环，阻塞等待新连接
    main_reactor->event_loop();

    sub_thread.join();
    delete main_reactor;
    delete sub_reactor;
    delete thread_pool;

    return 0;
}
```

代码解读:

这段代码的每一步都至关重要：

1. **`new ThreadPool(4)`**: 创建业务处理核心，所有耗时的计算都将在这里发生。
2. **`new Reactor()` & `std::thread`**: 这是“主-从”模式的体现。我们创建了 `sub_reactor`，并立即为它启动了一个全新的线程。从此，这个 `sub_reactor` 就活在了自己的世界里，专门负责处理已建立连接的 I/O。
3. **`new ListenHandler(9527, sub_reactor)`**: 这是整个连接分发机制的关键。在创建 `ListenHandler` (我们的“接待员”)时，我们明确告诉它：“你接受到的新连接，都应该交给 `sub_reactor` 去处理”。这个 `sub_reactor` 的指针被 `ListenHandler` 精心保存起来。
4. **`main_reactor->regist(...)`**: 将“接待员”注册到 `main_reactor`，并告诉它只关心 `READ` 事件。对于监听 `socket` 来说，`READ` 事件就意味着“有新的客人来了（新连接）”。
5. **`main_reactor->event_loop()`**: 主线程调用这一句后，便全身心投入到等待新连接的工作中，不再理会其他事情。

### 3.3. `reactor_impl.cpp`：Reactor 的心脏

`Reactor` 的核心是一个永不停歇的事件循环。它的生命就在于等待事件，然后分发事件。这个文件同时实现了 `Reactor` 接口和 `ReactorImplementation` 的具体逻辑，体现了Pimpl设计模式。

```
// 来自 reactor_impl.cpp
#include "reactor_impl.h"
#include "epoll_demultiplexer.h" // 默认使用 epoll
#include <iostream>

// ReactorImplementation 构造函数
ReactorImplementation::ReactorImplementation() : demux(new EpollDemultiplexer()) {}

// ReactorImplementation 事件循环
void ReactorImplementation::event_loop(int timeout) {
    while (true) {
        demux->wait_event(handlers, timeout);
    }
}

// ReactorImplementation 注册事件
void ReactorImplementation::regist(EventHandler* handler, Event evt) {
    std::lock_guard<std::mutex> lock(handlers_mutex);
    Handle fd = handler->get_handle();
    handlers[fd] = handler;
    demux->regist(fd, evt);
}

// ... remove 和 modify 的实现 ...

// Reactor 对外接口的实现，全部转发给 impl
Reactor::Reactor() : impl(new ReactorImplementation()) {}
Reactor::~Reactor() { delete impl; }
void Reactor::regist(EventHandler* handler, Event evt) { impl->regist(handler, evt); }
// ... remove 和 modify 的转发 ...
void Reactor::event_loop(int timeout) { impl->event_loop(timeout); }
```

**代码解读**:

- **`ReactorImplementation::ReactorImplementation()`**: 构造函数中，它实例化了一个 `EpollDemultiplexer`。这体现了面向接口编程的好处：如果想换成 `select`，只需要改动这一行代码。
- **`event_loop()`**: 这个 `while(true)` 循环就是 Reactor 线程的全部生命。`demux->wait_event(...)` 是一个阻塞调用，线程会“睡”在这里，直到有事件发生或者超时。
- **`regist()`**: 当需要注册一个新的事件处理器时，它首先用 `std::mutex` 锁住 `handlers`，保证线程安全，然后将 `fd` 和 `handler` 的映射关系存入 `map` 中，最后调用底层的 `demux` 去真正完成系统层面的事件注册。
- **`Reactor` 类**: 你可以看到，`Reactor` 类的方法都极其简单，它们只是将调用原封不动地转发给内部的 `impl` 指针。这就是 **Pimpl (Pointer to implementation)** 设计模式，它将接口与实现彻底分离，降低了编译依赖。

### 3.4. `epoll_demultiplexer.cpp`：高性能的事件感知

`epoll` 是 Reactor 模式在 Linux 平台下的高性能基石。它负责与操作系统内核打交道，告诉内核我们关心哪些事件，并高效地获取就绪事件。

```
// 来自 epoll_demultiplexer.cpp
bool EpollDemultiplexer::regist(Handle handle, Event evt) {
    struct epoll_event event;
    event.data.fd = handle;
    event.events = 0;

    if (evt & READ) event.events |= EPOLLIN;
    if (evt & WRITE) event.events |= EPOLLOUT;

    // 关键：设置为边缘触发模式
    event.events |= EPOLLET;

    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, handle, &event) < 0) {
        // ... 错误处理
        return false;
    }
    return true;
}

int EpollDemultiplexer::wait_event(std::map<Handle, EventHandler*>& handlers, int timeout) {
    int num_events = epoll_wait(epoll_fd, events.data(), static_cast<int>(events.size()), timeout);
    // ... 错误处理

    for (int i = 0; i < num_events; ++i) {
        Handle fd = events[i].data.fd;
        auto it = handlers.find(fd);
        if (it == handlers.end()) continue;
        EventHandler* handler = it->second;

        if (events[i].events & EPOLLIN) {
            handler->handle_read();
        }
        if (events[i].events & EPOLLOUT) {
            handler->handle_write();
        }
        // ... 其他事件处理
    }
    return num_events;
}
```

**代码解读**:

- **`regist()`**: 这里的核心是 `epoll_ctl` 系统调用。**最关键的一行是 `event.events |= EPOLLET;`**。它将 `epoll` 设置为**边缘触发(Edge-Triggered)模式。这意味着内核只在状态改变**时通知我们一次（比如数据从无到有），这要求我们在收到通知后必须一次性将所有数据读/写完。
- **`wait_event()`**: 之前在 `Reactor::event_loop` 中看到的阻塞点，最终就落在了 `epoll_wait` 这个系统调用上。当它返回时，`events` 数组中就包含了所有就绪的事件。我们遍历这个数组，根据 `fd` 找到对应的 `handler`，并根据事件类型（`EPOLLIN` 代表可读，`EPOLLOUT` 代表可写）调用相应的回调函数。**事件的分发和回调就在这里完成**。

### 3.5. `select_demultiplexer.cpp`：经典但备用的事件感知

这个文件提供了 `select` 版本的事件分离器。虽然在高性能场景下 `epoll` 是首选，但 `select` 作为 UNIX 系统中最古老、兼容性最好的 I/O 复用机制，它的存在展示了我们框架设计的灵活性。

```
// 来自 select_demultiplexer.cpp
int SelectDemultiplexer::wait_event(std::map<Handle, EventHandler*>& handlers, int timeout) {
    fd_set temp_read = read_set;
    fd_set temp_write = write_set;
    fd_set temp_err = err_set;
    timeval tv = { timeout / 1000, (timeout % 1000) * 1000 };
    int max_fd = handlers.empty() ? 0 : handlers.rbegin()->first;

    int num_events = select(max_fd + 1, &temp_read, &temp_write, &temp_err, timeout == 0 ? nullptr : &tv);
    
    if (num_events > 0) {
        for (auto const& [fd, handler] : handlers) {
            if (FD_ISSET(fd, &temp_read)) {
                handler->handle_read();
            }
            if (FD_ISSET(fd, &temp_write)) {
                handler->handle_write();
            }
            // ... 错误处理
        }
    }
    return num_events;
}

bool SelectDemultiplexer::regist(Handle handle, Event evt) {
    if (evt & READ) FD_SET(handle, &read_set);
    if (evt & WRITE) FD_SET(handle, &write_set);
    FD_SET(handle, &err_set);
    return true;
}
```

**代码解读**:

- 与 `epoll` 不同，`select` 的工作方式是“轮询”。每次调用 `select` 前，都需要重新构建 `fd_set`（所以要用 `temp_read` 等副本），并且在返回后，需要遍历所有注册过的 `fd` 来检查是哪一个就绪了。
- 这种遍历所有 `fd` 的方式，使得 `select` 的性能会随着连接数的增加而线性下降。但它的代码逻辑相对直观，并且在任何 POSIX 系统上都可用。在我们的框架中，只需要修改 `ReactorImplementation` 的构造函数，就能无缝切换到 `select`，这正是抽象设计的魅力所在。

### 3.6. `listen_handler.cpp`：“接待员”的诞生与职责

当 Reactor 侦测到事件后，具体的响应逻辑由 `EventHandler` 的派生类来执行。`ListenHandler` 是第一个响应者。

```
// 来自 listen_handler.cpp
ListenHandler::ListenHandler(int port, Reactor* sub) : sub_reactor(sub) {
    listen_fd = socket(AF_INET, SOCK_STREAM, 0);
    // ... setsockopt, bind, listen 等一系列 socket 初始化操作 ...
}

void ListenHandler::handle_read() {
    sockaddr_in client_addr;
    socklen_t addr_len = sizeof(client_addr);
    Handle client_fd = accept(listen_fd, (struct sockaddr*)&client_addr, &addr_len);

    if (client_fd > 0) {
        set_non_blocking(client_fd); // 必须设置为非阻塞
        
        // 创建新的 SockHandler 来处理这个连接
        SockHandler* client_handler = new SockHandler(client_fd, sub_reactor);
        
        // 将新的 handler 注册到 Sub Reactor
        sub_reactor->regist(client_handler, READ);
    }
}
```

**代码解读**:

- **构造函数**: 完成了标准的网络服务启动流程：创建 `socket`，设置 `SO_REUSEADDR` 选项，`bind` 到指定端口，然后 `listen`。同时，它把从 `server.cpp` 传来的 `sub_reactor` 指针保存起来。
- **`handle_read()`**: 这是它的核心。当 Main Reactor 通知它有 `READ` 事件时，它知道这意味着有新连接。它通过 `accept()` 获得新的 `client_fd`。然后，它做了两件至关重要的事：创建 `SockHandler`，并调用 `sub_reactor` 的 `regist` 方法，将这个新连接的后续处理权完全交接给了 Sub Reactor。

### 3.7. `sock_handler.cpp`：数据的“通讯员”

这是最繁忙的角色，负责具体的数据收发，并作为 I/O 线程与业务线程（线程池）之间的桥梁。

```
// 来自 sock_handler.cpp
void SockHandler::handle_read() {
    std::string received_data;
    // ... 使用 while(true) + recv 循环读取数据，以适应 ET 模式 ...

    if (!received_data.empty()) {
        thread_pool->enqueue([this, data = std::move(received_data)]() {
            // ... 在线程池中模拟业务处理 ...
            std::string processed = data + " [processed by epoll]";
            {
                std::lock_guard<std::mutex> lock(buf_mutex);
                write_buf += processed;
            }
            // 通知 Reactor 修改事件为可写
            reactor->modify(sock_fd, WRITE);
        });
    }
}

void SockHandler::handle_write() {
    // ... 从 write_buf 中取出待发送数据 to_send ...
    // ... 使用 while(true) + send 循环发送数据，以适应 ET 模式 ...
    
    // 所有数据都已发送完毕，切回监听读事件
    reactor->modify(sock_fd, READ);
}
```

**代码解读**:

- **`handle_read()`**:
  1. **循环读取**: `while (true)` 循环是为了正确处理 ET 模式。我们必须不停地 `recv`，直到内核缓冲区被榨干。
  2. **任务投递**: 读取完数据后，I/O 线程（Sub Reactor 线程）的工作就完成了。它将数据和处理逻辑打包成一个 `lambda` 表达式，通过 `thread_pool->enqueue` 扔进线程池。
  3. **状态变更**: 在线程池的 `lambda` 中，业务处理完成后，调用 `reactor->modify(sock_fd, WRITE)`。**这是从业务线程回到 I/O 线程的关键一步**，它通知 Sub Reactor 准备进行写操作。
- **`handle_write()`**:
  1. **循环写入**: 同样因为 ET 模式，我们要尽可能多地 `send` 数据，直到数据发完或者缓冲区再次被写满。
  2. **状态复位**: 数据全部发送完毕后，我们再次调用 `modify`，将事件改回 `READ`，以便接收客户端的下一个请求。这就完成了一个完整的请求-响应循环。

### 3.8. `thread_pool.cpp`：业务处理的“加工厂”

这是一个标准的生产者-消费者模型的线程池实现，是解耦 I/O 和计算的关键。它让 I/O 线程可以“随收随走”，不必等待耗时的业务计算。

```
// 来自 thread_pool.cpp
ThreadPool::ThreadPool(size_t threads) : stop(false) {
    for (size_t i = 0; i < threads; ++i) {
        workers.emplace_back([this] {
            while (true) {
                std::function<void()> task;
                {
                    std::unique_lock<std::mutex> lock(queue_mutex);
                    condition.wait(lock, [this] { return stop || !tasks.empty(); });
                    if (stop && tasks.empty()) return;
                    task = std::move(tasks.front());
                    tasks.pop();
                }
                task();
            }
        });
    }
}

void ThreadPool::enqueue(std::function<void()> task) {
    {
        std::unique_lock<std::mutex> lock(queue_mutex);
        if (stop) return;
        tasks.emplace(task);
    }
    condition.notify_one();
}
```

**代码解读**:

- **工作线程循环**: 每个工作线程都运行在一个 `while(true)` 循环中。`condition.wait(lock, ...)` 是核心，它会原子地释放锁并让线程休眠，直到被 `notify` 唤醒。
- **`enqueue()`**: 这是生产者接口。当 `SockHandler` 需要提交任务时，它调用此函数。函数将任务放入队列，然后调用 `condition.notify_one()` 来“踢”一个正在 `wait` 中沉睡的线程，告诉它“有活了，快起来干”。

### 3.9. `client.cpp`：一个简单的测试工具

要检验我们复杂的服务器，需要一个客户端。这个文件实现了一个简单的、阻塞式的、交互的 TCP 客户端。

```
// 来自 client.cpp
int run_client() {
    int sock = socket(AF_INET, SOCK_STREAM, 0);
    // ... 设置服务器地址 ...
    
    if (connect(sock, (struct sockaddr*)&serv_addr, sizeof(serv_addr)) < 0) {
        // ... 连接失败处理 ...
    }

    std::cout << "Connected to server. Type 'exit' to quit." << std::endl;

    while (true) {
        std::string message;
        std::cout << "> ";
        std::getline(std::cin, message);

        if (message == "exit" || std::cin.eof()) break;

        send(sock, message.c_str(), message.length(), 0);

        char buffer[1024] = "";
        recv(sock, buffer, 1024, 0);
        // ... 打印接收到的数据 ...
    }

    close(sock);
    return 0;
}
```

代码解读:

这段代码的逻辑非常直接：

1. 创建 `socket` 并 `connect` 到服务器。

2. 进入一个 `while` 循环，从标准输入（`std::cin`）读取用户输入的一行文字。

3. 使用 `send` 将文字发送给服务器。

4. 使用 `recv` 阻塞地等待服务器的响应。

5. 打印响应，然后继续下一次循环。

   它很简单，但足以用来测试我们服务器的正确性和并发处理能力。

## 4. 总结

通过对每一个 `cpp` 文件的逐一剖析，我们得以窥见这个高性能服务器的全貌。从程序的入口，到服务架构的搭建，再到事件的感知、分发、响应，最后到业务逻辑的异步处理，每一部分代码都各司其职，又紧密协作，共同构成了一个清晰、高效、可扩展的系统。希望这份详尽的文档，能让你对这个项目的设计与实现有一个更具体、更深刻的认识。